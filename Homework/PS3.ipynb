{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3\n",
    "\n",
    "For this problem set, you will expand on PS2 to perform and evaluate various sentiment classification methods.\n",
    "\n",
    "Your name: Ashley Cortez\n",
    "\n",
    "You abc123: gql524\n",
    "\n",
    "## Submission Instructions\n",
    "\n",
    "After completing the exercises below, generate a pdf of the code **with** outputs. After that create a zip file containing both the completed exercise and the generated PDF/HTML. You are **required** to check the PDF/HTML to make sure all the code **and** outputs are clearly visible and easy to read. If your code goes off the page, you should reduce the line size. I generally recommend not going over 80 characters.\n",
    "\n",
    "Finally, name the zip file using a combination of your the assigment and your name, e.g., ps3_rios.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (1 point)\n",
    "\n",
    "For this step, you will load the training and test sentiment datasets \"twitdata_TEST.tsv\" and \"allTrainingData.tsv\". The data should be loaded into 4 lists of strings: X_txt_train, X_txt_test, y_test, y_train.\n",
    "\n",
    "Note, when using csvreader, you need to pass the \"quoting\" the value csv.QUOTE_NONE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "file_train = open(\"allTrainingData.tsv\")\n",
    "reader_train = csv.reader(file_train,delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "\n",
    "X_txt_train = []\n",
    "y_train = []\n",
    "\n",
    "for row in reader_train:\n",
    "    y= row[2]\n",
    "    X_txt = \"\\t\".join(row[3:])\n",
    "    X_txt_train.append(X_txt)\n",
    "    y_train.append(y)\n",
    "\n",
    "\n",
    "file_test = open(\"twitdata_TEST.tsv\")\n",
    "reader_test = csv.reader(file_test,delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "\n",
    "X_txt_test = []\n",
    "y_test = []\n",
    "\n",
    "for row in reader_test:\n",
    "    y = row[2]\n",
    "    X_txt = \"\\t\".join(row[3:])\n",
    "    X_txt_test.append(X_txt)\n",
    "    y_test.append(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(type(X_txt_train) == type(list()))\n",
    "assert(type(X_txt_train[0]) == type(str()))\n",
    "assert(type(X_txt_test) == type(list()))\n",
    "assert(type(X_txt_test[0]) == type(str()))\n",
    "assert(type(y_test) == type(list()))\n",
    "assert(type(y_train) == type(list()))\n",
    "assert(len(X_txt_test) == 3199)\n",
    "assert(len(y_test) == 3199)\n",
    "assert(len(X_txt_train) == 8018)\n",
    "assert(len(y_train) == 8018)\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (2 point)\n",
    "\n",
    "This part is similar to HW2 (using the positive_words and negative_words variables). We will compare last homework's lexicon-based classification method with supervised models. Only make predictions on the test split and store all predictions in the list lex_test_preds. Next, calculate the **macro** precision, macro recall, and macro f1 scores using the lex_test_preds list.\n",
    "\n",
    "You can learn more about lexicon-based classification in Chapter 19.6. If you are interested, the chapter is available online for free at the following link: [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/19.pdf)\n",
    "\n",
    "Also, note that I have wrote the code for this exercise, you job is to read the code, understand it, and apply it to make the predictions.\n",
    "\n",
    "**INTUITION:** For PS2 you implemented a \"lexicon-based classifier\". You looked at a few examples and manually accessed it's performance. Howeover, that was arbitary. Now we want to see how well it actually works. Hence, in this homework (for this exercise), you will use the class I provided that implements the lexicon-based classifier and use the provided \"annotatoed dataset\" (loaded in Exercise 1) to see how well it performance. If it worked 100\\% accurately, the F1 would be 1.00. However, it does not, so you should expect a smaller score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THE CODE IN THIS CELL\n",
    "class LexiconClassifier():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            Initalize the Lexicon classifer by loading lexicons. \n",
    "        \"\"\"\n",
    "        self.positive_words = set()\n",
    "        with open('positive-words.txt', encoding = 'utf-8') as iFile:\n",
    "            for row in iFile:\n",
    "                self.positive_words.add(row.strip())\n",
    "\n",
    "        self.negative_words = set()\n",
    "        with open('negative-words.txt', encoding='iso-8859-1') as iFile:\n",
    "            for row in iFile:\n",
    "                self.negative_words.add(row.strip())\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        \"\"\"\n",
    "            Returns a sentiment prediction give an input string.\n",
    "            \n",
    "            Keyword arguments:\n",
    "            sentence -- string (e.g., \"This is good good good\")\n",
    "            \n",
    "            Returns:\n",
    "            pred -- a string (\"postive, \"negative\", or \"neutral\")\n",
    "        \"\"\"\n",
    "        num_pos_words = 0\n",
    "        num_neg_words = 0\n",
    "        for word in sentence.lower().split():\n",
    "            if word in self.positive_words:\n",
    "                num_pos_words += 1\n",
    "            elif word in self.negative_words:\n",
    "                num_neg_words += 1\n",
    "        \n",
    "        pred = 'neutral'        \n",
    "        if num_pos_words > num_neg_words:\n",
    "            pred = 'positive'\n",
    "        elif num_pos_words < num_neg_words:\n",
    "            pred = 'negative'\n",
    "            \n",
    "        return pred\n",
    "    \n",
    "    def count_pos_words(self, sentence):\n",
    "        \"\"\"\n",
    "            Returns the number of positive words in string\n",
    "            \n",
    "            Keyword arguments:\n",
    "            sentence -- string (e.g., \"This is good good good\")\n",
    "            \n",
    "            Returns:\n",
    "            pred -- an integer (e.g., 3)\n",
    "        \"\"\"\n",
    "        num_pos_words = 0\n",
    "        for word in sentence.lower().split():\n",
    "            if word in self.positive_words:\n",
    "                num_pos_words += 1\n",
    "        return num_pos_words\n",
    "\n",
    "    def count_neg_words(self, sentence):\n",
    "        \"\"\"\n",
    "            Returns the number of negative words in string\n",
    "            \n",
    "            Keyword arguments:\n",
    "            sentence -- string (e.g., \"This is good good good\")\n",
    "            \n",
    "            Returns:\n",
    "            pred -- an integer (e.g., 3)\n",
    "        \"\"\"\n",
    "        num_neg_words = 0\n",
    "        for word in sentence.lower().split():\n",
    "            if word in self.negative_words:\n",
    "                num_neg_words += 1\n",
    "        return num_neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5500\n",
      "Recall: 0.5443\n",
      "F1: 0.5452\n"
     ]
    }
   ],
   "source": [
    "# WRITE CODE HERE\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "    \n",
    "lex_test_preds = [] # Initialize this as an empty list\n",
    "\n",
    "LC = LexiconClassifier()\n",
    "# Loop over X_txt_test\n",
    "#    for each string in X_txt_test (i.e., for each item in the list), pass it to LexiconClassifiers .predict() method\n",
    "#    append the prediction to lex_test_preds\n",
    "\n",
    "for string in X_txt_test:\n",
    "    lex_test_preds.append(LC.predict(string))\n",
    "    \n",
    "\n",
    "precision = precision_score(y_test, lex_test_preds, average = 'macro')\n",
    "recall = recall_score(y_test, lex_test_preds, average = 'macro') \n",
    "f1 = f1_score(y_test, lex_test_preds, average = 'macro') \n",
    "                          \n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(type(lex_test_preds) == type(list()))\n",
    "assert(type(lex_test_preds[0]) == type(str()))\n",
    "assert(set(lex_test_preds) == set([\"positive\", \"negative\", \"neutral\"]))\n",
    "assert(len(lex_test_preds) == len(y_test))\n",
    "assert(type(precision) == type(float()) or type(precision) == type(np.float64()))\n",
    "assert(type(recall) == type(float()) or type(recall) == type(np.float64()))\n",
    "assert(type(f1) == type(float()) or type(f1) == type(np.float64()))\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (1 point)\n",
    "\n",
    "Again, using the LexiconClassifier, write code to generate a lists of lists where each sublist contains the number of positive words and negative words in a tweet. For example, assume we are given the following train and test datasets\n",
    "\n",
    "``` python\n",
    "X_txt_train = [\"good good\", \"bad bad\"]\n",
    "X_txt_test = [\"great\", \"bad bad great\"]\n",
    "```\n",
    "\n",
    "you should write code that creates two lists of lists as follows:\n",
    "\n",
    "``` python\n",
    "X_train_lexicon_features = [[2, 0], [0,2]] # [2, 0] means the first tweeta has 2 positive words and 0 negative words.\n",
    "X_test_lexicon_features = [[1, 0], [1, 2]]\n",
    "```\n",
    "\n",
    "Why are we doing this? We will use these as addition features in Exercise 5, combining it with the ngram features. Combining different sets of features is called \"Feature Engineering\" and is one of the most important steps of many machine learning tasks. In this case, we are using the lexicons to generate additional features. But, we could also count the number of capitalized words, number of punctuation marks, etc. We would come up with different feature sets via trial-and-error. We can guess what type of features would help our task. For instance, for sentiment prediction, we may guess that having many capitalized words is predictive of something negative (e.g., \"WHY ARE YOU DOING THIS!!!!!\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE\n",
    "\n",
    "X_train_lexicon_features = [] # Initailze to an empty list. This will be a list of lists\n",
    "X_test_lexicon_features = [] #  Initailze to an empty list. This will be a list of lists\n",
    "\n",
    "# Loop over X_txt_test\n",
    "#    for each string in X_txt_test (i.e., for each item in the list), pass it to LexiconClassifiers .count_pos_words() and count_neg_words method\n",
    "#    append a list with the counts to X_test_lexicon_features\n",
    "\n",
    "for string in X_txt_test:\n",
    "    X_test_lexicon_features.append([LC.count_pos_words(string), \n",
    "    LC.count_neg_words(string)])\n",
    "    \n",
    "# Loop over X_txt_train\n",
    "#    for each string in X_txt_train (i.e., for each item in the list), pass it to LexiconClassifiers .count_pos_words() and count_neg_words method\n",
    "#    append a list with the counts to X_train_lexicon_features\n",
    "\n",
    "for string in X_txt_train:\n",
    "    X_train_lexicon_features.append([LC.count_pos_words(string), \n",
    "    LC.count_neg_words(string)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(type(X_train_lexicon_features) == type(list()))\n",
    "assert(type(X_test_lexicon_features) == type(list()))\n",
    "assert(type(X_test_lexicon_features[0]) == type(list()))\n",
    "assert(len(X_train_lexicon_features) == len(X_txt_train))\n",
    "assert(len(X_test_lexicon_features) == len(X_txt_test))\n",
    "assert(len(X_train_lexicon_features[0]) == 2)\n",
    "assert(len(X_test_lexicon_features[0]) == 2)\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 (2 points)\n",
    "\n",
    "For this task you should creat a feature matrix using CountVectorizer and train a LinearSVC model from scikit-learn. On the train split, use GridSearchCV to find the best LinearSVC C values (0.0001, 0.001, 0.001, 0.01, 0.1, 1, 10, or 100) based on the **macro** f1 scoring metric (hint: \"macro\" average) and set the cv parameter to 5. Also, with the CountVectorizer, only use unigrams (i.e., set ngram_range = (1,1)). Note that GridSearchCV will retrain the final classifier using the best parameters, so you don't need to do it manually.\n",
    "\n",
    "**INTUITION:** For this exercise, you are implementing a simple linear model using bag-of-words features. This is generally a very strong and simple baseline for text classification. Compare the scores from this exercise to the results in Exercise 2. You will find that the machine learning-based model implemented here acheives better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1: 0.6593\n",
      "Precision: 0.6525\n",
      "Recall: 0.5740\n",
      "F1: 0.5878\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "vec = CountVectorizer(ngram_range = (1,1))\n",
    "\n",
    "X_train = vec.fit_transform(X_txt_train) \n",
    "X_test = vec.transform(X_txt_test)\n",
    "\n",
    "svc = LinearSVC()\n",
    "params = {'C':[0.0001, 0.001, 0.01, 0.1, 1., 10, 100]}\n",
    "clf = GridSearchCV(svc, params, cv = 5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "validation_score = clf.best_score_ # Get the score from the GridSearchCV \"best score\"\n",
    "print(\"Validation F1: {:.4f}\".format(validation_score))\n",
    "\n",
    "svm_test_predictions = clf.predict(X_test) # \"predict\" on X_test \n",
    "\n",
    "precision = precision_score(y_test, svm_test_predictions, average = 'macro') # Get scores using svm_test_predictions and y_test with the precision_score method\n",
    "recall = recall_score(y_test, svm_test_predictions, average = 'macro') \n",
    "f1 = f1_score(y_test, svm_test_predictions, average = 'macro') \n",
    "\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "assert(type(X_train) == type(csr_matrix(0)) or type(X_train) == type(np.array(0)))\n",
    "assert(type(X_test) == type(csr_matrix(0)) or type(X_test) == type(np.array(0)))\n",
    "assert(X_train.shape[0] == len(X_txt_train))\n",
    "assert(X_test.shape[0] == len(X_txt_test))\n",
    "assert(X_train.shape[1] == X_test.shape[1])\n",
    "assert(type(precision) == type(float()) or type(precision) == type(np.float64()))\n",
    "assert(type(recall) == type(float()) or type(recall) == type(np.float64()))\n",
    "assert(type(f1) == type(float()) or type(f1) == type(np.float64()))\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 (2 points)\n",
    "\n",
    "Repeat the experiment from exercise 4, but include the lexicon features (from exercise 3) with the CountVectorizer features. Specifically, you need to concatenate the variables ```X_train_lexicon_features``` and ```X_test_lexicon_features``` with ```X_train``` and ```X_test```, respectively. Intuitively, we are performing feature engineering by adding \"lexicon features\".\n",
    "\n",
    "HINT: You will need to convert the lexicon features to numpy arrays then call hstack from the scipy.sparse library (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.hstack.html)\n",
    "\n",
    "```python\n",
    "    from scipy.sparse import hstack\n",
    "    new_matrix = hstack([ngram_matrix, lexicon_matrix])\n",
    "```\n",
    "\n",
    "Again, this is a \"Feature Engineering\" exercise. This is common in all machine learning tasks.\n",
    "\n",
    "**INTUITION:** How do we improve the model Exercise 4? You could try different machine learning models. However, it is generally better to focus on feature engineering. What information can you provide the model to make better predictions? Here we will try to combine counts using the Lexicon from Exercise 3 as additional features (i.e., combined with the bag-of-word features) from Exercise 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "b = [4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "print(np.dot(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1: 0.6729\n",
      "Precision: 0.6894\n",
      "Recall: 0.5719\n",
      "F1: 0.5875\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# WRITE CODE HERE\n",
    "\n",
    "# Summary:\n",
    "# 1. Convert X_txt_train and X_txt_test to matricies of numbers (i.e., use CountVectorizer)\n",
    "\n",
    "vec = CountVectorizer(ngram_range = (1,1))\n",
    "\n",
    "X_train_w_lex = vec.fit_transform(X_txt_train) # This will be the matrix from CountVectorizer (X_txt_train)\n",
    "X_test_w_lex = vec.transform(X_txt_test)\n",
    "\n",
    "# Now we need to convert X_train_lexicon_features and X_test_lexicon_features to numpy arrays\n",
    "\n",
    "# \"hstack\" X_train_lexicon_features with X_train_w_lex\n",
    "\n",
    "X_train_w_lex = sp.hstack([X_train_lexicon_features, X_train_w_lex]).toarray()\n",
    "\n",
    "# \"hstack\" X_test_lexicon_features with X_test_w_lex\n",
    "\n",
    "X_test_w_lex = sp.hstack([X_test_lexicon_features, X_test_w_lex]).toarray()\n",
    "\n",
    "# Initialize the classifier LinearSVC \n",
    "\n",
    "svc = LinearSVC()\n",
    "\n",
    "# Create the params with the C values\n",
    "\n",
    "params = {'C':[0.0001, 0.001, 0.01, 0.1, 1., 10, 100]}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "\n",
    "clf = GridSearchCV(svc, params, cv = 5)\n",
    "\n",
    "# \"fit\" the model  on X_train_w_lex\n",
    "\n",
    "clf.fit(X_train_w_lex, y_train)\n",
    "\n",
    "\n",
    "validation_score = clf.best_score_\n",
    "print(\"Validation F1: {:.4f}\".format(validation_score))\n",
    "\n",
    "svm_lex_test_predictions = clf.predict(X_test_w_lex) # Get predictions on X_test_w_lex\n",
    "\n",
    "precision = precision_score(y_test, svm_lex_test_predictions, average = 'macro') # Get scores using svm_test_predictions and y_test with the precision_score method\n",
    "recall = recall_score(y_test, svm_lex_test_predictions, average = 'macro') \n",
    "f1 = f1_score(y_test, svm_lex_test_predictions, average = 'macro') \n",
    "\n",
    "\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "assert(X_train_w_lex.shape[0] == len(X_txt_train))\n",
    "assert(X_test.shape[0] == len(X_txt_test))\n",
    "assert(X_train_w_lex.shape[1] == X_test.shape[1] + 2)\n",
    "assert(X_train_w_lex.shape[1] == X_test_w_lex.shape[1])\n",
    "assert(type(precision) == type(float()) or type(precision) == type(np.float64()))\n",
    "assert(type(recall) == type(float()) or type(recall) == type(np.float64()))\n",
    "assert(type(f1) == type(float()) or type(f1) == type(np.float64()))\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 (1 point)\n",
    "\n",
    "For this exercise, you will perform manual analysis of the predictions. Answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Musical awareness: Great Big Beautiful Tomorrow has an ending, Now is the time does not\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "Tweet: On Radio786 100.4fm 7:10 Fri Oct 19 Labour analyst Shawn Hattingh: Cosatu's role in the context of unrest in the mining http://t.co/46pjzzl6\n",
      "Ground-Truth Class: neutral\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: negative\n",
      "\n",
      "Tweet: Kapan sih lo ngebuktiin,jan ngomong doang Susah Susah.usaha Aja blm udh nyerah,inget.if you never try you'll never know.cowok kok gentle bgt\n",
      "Ground-Truth Class: negative\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "Tweet: Tomorrow come and hear @DavidWillettsMP&amp;@MASieghart debate \"Navigating the new Higher Education market\" 5.30pm, Jurys Inn #CPC12\n",
      "Ground-Truth Class: neutral\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: Excuse the connectivity of this live stream, from Baba Amr, so many activists using only one Sat Modem. LIVE http://t.co/U283IhZ5 #Homs\n",
      "Ground-Truth Class: neutral\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: negative\n",
      "\n",
      "Tweet: Show your LOVE for your local field &amp; it might win an award!  Gallagher Park #Bedlington current 4th in National Award http://t.co/WeiMDtQt\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "Tweet: @firecore Can you tell me when an update for the Apple TV 3rd gen becomes available? The missing update holds me back from buying #appletv3\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: @Heavensbasement The Crown, Filthy McNastys, Katy Dalys or the Duke if York in Belfast! Can't wait to catch you guys tomorrow night!\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: negative\n",
      "\n",
      "Tweet: Uncover the Eternal City! Return flights to Rome travel on the 21st January, for 3 nights Augustea, 3 star Hotel... http://t.co/tw0Jeh9g\n",
      "Ground-Truth Class: neutral\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: My #cre blog Oklahoma Per Square Foot returns to the @JournalRecord blog hub tomorrow. I will have some interesting local data to share.\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "Tweet: \"@bbcburnsy: Loads from SB; talks with Chester continue; no deals 4 out of contract players 'til Jan; Dev t Roth ,Coops to Chest'ld #hcafc\"\n",
      "Ground-Truth Class: negative\n",
      "SVM Prediction: negative\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: Trey Burke has been suspended for the Northern Michigan game (exhibition) tomorrow. http://t.co/oefkAElW\n",
      "Ground-Truth Class: negative\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: W.O.W Wednesday!Marni lands this Lumberjack vest for the ladies looking to bring a little Tom boy toughness  http://t.co/7NyCbdJR\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: negative\n",
      "\n",
      "Tweet: Activists in Deir Ezzor captured this image of Musab Bin Umair Mosque after regime forces set it on fire Wednesday. http://t.co/MRcoprCE\n",
      "Ground-Truth Class: negative\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: @karaotr You will appreciate this.. Sunday brunch coffee: Normal cup in b/g and then the BOWL of java. Yowza. http://t.co/XhbtaCvm\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "Tweet: Join me Wed for a live webcast on cost optimization for IT, for the SMB crowd. http://t.co/tyJn4RES  &lt;&lt; send your questions in! #DellWebcast\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: Special THANKS to EVERYONE for coming out to Taboo Tuesday With DST tonight! It was FUN&amp;educational!!! :) @XiEtaDST\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: negative\n",
      "\n",
      "Tweet: @fatimasule That was the revelation I mentioned on sunday evening. I am still in Abj. How are u &amp; where have u been again?\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "Tweet: Kim Hyung Jun - Football Team the 2nd A Match at YeongDeungPo-gu DaeRimDong [12.10.27] Credit : tlxhah #6 http://t.co/u7mPTl0X\n",
      "Ground-Truth Class: neutral\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: The audio booth is ready to blow the roof off the Comcast Center tomorrow! Are you? #MDMadness http://t.co/B19fECgY\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_tweets = 0\n",
    "for text, svm_pred, svm_lex_pred, lex_pred, y  in zip(X_txt_test, svm_test_predictions, svm_lex_test_predictions, lex_test_preds, y_test):\n",
    "    print(\"Tweet: {}\".format(text))\n",
    "    print(\"Ground-Truth Class: {}\".format(y))\n",
    "    print(\"SVM Prediction: {}\".format(svm_pred))\n",
    "    print(\"SVM+Lexicon Prediction: {}\".format(svm_lex_pred))\n",
    "    print(\"Lexicon Model Prediction: {}\".format(lex_pred))\n",
    "    print()\n",
    "    \n",
    "    num_tweets += 1\n",
    "    if num_tweets == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following tasks:\n",
    " \n",
    "- Manually annotate all of the tweets printed above:\n",
    "   1. Tweet 1 Sentiment: Positive\n",
    "   1. Tweet 2 Sentiment: Neutral \n",
    "   1. Tweet 3 Sentiment: Neutral\n",
    "   1. Tweet 4 Sentiment: Neutral\n",
    "   1. Tweet 5 Sentiment: Neutral\n",
    "   1. Tweet 6 Sentiment: Positive\n",
    "   1. Tweet 7 Sentiment: Neutral\n",
    "   1. Tweet 8 Sentiment: Positive\n",
    "   1. Tweet 9 Sentiment: Neutral\n",
    "   1. Tweet 10 Sentiment: Positive\n",
    "   1. Tweet 11 Sentiment: Negative\n",
    "   1. Tweet 12 Sentiment: Negative\n",
    "   1. Tweet 13 Sentiment: Positive\n",
    "   1. Tweet 14 Sentiment: Neutral\n",
    "   1. Tweet 15 Sentiment: Positive\n",
    "   1. Tweet 16 Sentiment: Neutral\n",
    "   1. Tweet 17 Sentiment: Positive\n",
    "   1. Tweet 18 Sentiment: Neutral \n",
    "   1. Tweet 19 Sentiment: Neutral \n",
    "   1. Tweet 20 Sentiment: Neutral\n",
    "\n",
    "- How many of your annotations match the ground truth labels? Do you think the datasets labels are correct? (Use your intuition)\n",
    "    - about 18 of my annotations match the ground truth labels. I think the labels are subjective, but mostly represent the \"correct\" label given the majority. \n",
    "\n",
    "- How many of your annotations match the lexicon-based model's predictions?\n",
    "    - about 15 \n",
    "\n",
    "- How many of your annotations match the SVM's predictions?\n",
    "    - about 18 \n",
    "    \n",
    "- How many of your annotations match the SVM+Lexicon's predictions?\n",
    "    - about 14\n",
    "    \n",
    "- Do you see any major limitations of the linear SVM model? Use your intuition, I will accept most answers, as long as it makes some sense. Please describe and provide examples below:\n",
    "     - a limitation of the SVM model is that it is not suitable for large datasets, and SVM does not perform very     well when the data set has more noise i.e. target classes are overlapping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 (1 point)\n",
    "\n",
    "For this exercise, you should come up with 10 other potential features that could be useful for sentiment analysis. You do not need to implement them. You simply need to list this. Make sure it is easy for me to understand the feature you describe. An example could be \"The count of the number of capitalized words in the text\".\n",
    "\n",
    "1. Idea 1 - speed with automation to score sentiments\n",
    "2. Idea 2 - identify any emojis in text \n",
    "3. Idea 3 - multilingual - being able to score in different languages  \n",
    "4. Idea 4 - social media - looking at captions and hashtags to determine sentiment\n",
    "5. Idea 5 - multimedia - analyze videos such as reels and tiktoks \n",
    "6. Idea 6 - automation to sort data into sentiment categories: positive, negative, neutral\n",
    "7. Idea 7 - real time analysis so companies can quickly address negative/positive comments\n",
    "8. Idea 8 - scoring for APIs to help accuracy - similar to one hot coding\n",
    "9. Idea 9 - dashboard to very insight statistics - helps read/view sentiments quickly \n",
    "10. Idea 10 - analyze brand monitoring - see where else your business is tagged outside of twitter, ie. Reddit, Facebook etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit 1 (2 points)\n",
    "\n",
    "For this extra credit the only goal is to improve your model on the test set (i.e., increase the **macro** f1 score). You may create new features, grid search over more parameters, try different feature weighting methods (e.g., TfidfVectorizer), or test different machine learning models. You can do whatever you want as long as the final test score improves, I will provide you with the extra credit points.\n",
    "\n",
    "DO NOT TRAIN ON THE TEST SET. That is cheating!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 216, in __call__\n",
      "    return self._score(\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 264, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 1123, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 1261, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 1544, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 1365, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 216, in __call__\n",
      "    return self._score(\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 264, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 1123, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 1261, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 1544, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 1365, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 216, in __call__\n",
      "    return self._score(\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 264, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 1123, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 1261, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 1544, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 1365, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 216, in __call__\n",
      "    return self._score(\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 264, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 1123, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 1261, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 1544, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py\", line 1365, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/ashley/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1: nan\n",
      "Precision: 0.6226\n",
      "Recall: 0.5261\n",
      "F1: 0.5304\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "params = {'n_estimators': [10, 100]}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "clf_rf = GridSearchCV(rf, params, scoring = 'f1', cv = 2)\n",
    "\n",
    "clf_rf.fit(X_train_w_lex, y_train)\n",
    "\n",
    "\n",
    "validation_score = clf_rf.best_score_\n",
    "print(\"Validation F1: {:.4f}\".format(validation_score))\n",
    "\n",
    "svm_lex_test_predictions_rf = clf_rf.predict(X_test_w_lex) \n",
    "\n",
    "precision = precision_score(y_test, svm_lex_test_predictions_rf, average = 'macro') \n",
    "recall = recall_score(y_test, svm_lex_test_predictions_rf, average = 'macro') \n",
    "f1 = f1_score(y_test, svm_lex_test_predictions_rf, average = 'macro') \n",
    "\n",
    "\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1: {:.4f}\".format(f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
